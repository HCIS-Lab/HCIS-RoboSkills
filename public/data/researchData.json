{
  "lab": {
    "name": "HCIS Lab",
    "fullName": "Human-Centered Intelligent Systems Lab",
    "institution": "National Yang Ming Chiao Tung University",
    "department": "Department of Computer Science",
    "pi": {
      "name": "Yi-Ting Chen",
      "title": "Associate Professor",
      "email": "ychen [at] cs.nycu.edu.tw",
      "links": {
        "cv": "https://drive.google.com/file/d/1LOtGzv_tZDOLgHGKkwYDWtCMZv_y8gUp/view",
        "googleScholar": "https://scholar.google.com/citations?user=8tRH7RMAAAAJ&hl=en",
        "dblp": "https://dblp.org/pid/12/5268-1.html"
      }
    },
    "philosophy": "Developing Human-Centered Physical AI Systems that bridge human needs with autonomous system capabilities"
  },
  "researchAreas": [
    {
      "id": "intelligent-driving",
      "name": "Intelligent Driving Systems",
      "description": "Developing intelligent systems that understand driving scenes, assess risks, and enable safer autonomous driving through human-centric design principles.",
      "color": "#5dade2",
      "icon": "car",
      "focusAreas": [
        {
          "id": "scene-understanding",
          "name": "Dynamic Scene Understanding & Risk Assessment",
          "description": "Understanding driving scenes and identifying risk objects through driver-centric approaches and causal reasoning."
        },
        {
          "id": "scenario-retrieval",
          "name": "Large-scale Scenario Retrieval Systems",
          "description": "Action-centric representation learning for fine-grained traffic scenario understanding and retrieval."
        },
        {
          "id": "3d-analysis",
          "name": "3D Dynamic Scene Analysis & Reconstruction",
          "description": "Real-time 3D scene analysis, object detection, and SLAM for autonomous systems."
        }
      ],
      "publications": [
        {
          "title": "Potential Field as Scene Affordance for Behavior Change-Based Visual Risk Object Identification",
          "venue": "ICRA 2025",
          "authors": [
            "Pang-Yuan Pao",
            "Shu-Wei Lu",
            "Ze-Yan Lu",
            "Yi-Ting Chen"
          ],
          "links": {
            "paper": "https://arxiv.org/pdf/2409.15846",
            "projectPage": "https://hcis-lab.github.io/PF-BCP/",
            "code": "https://github.com/HCIS-Lab/PF-BCP"
          }
        },
        {
          "title": "RiskBench: A Scenario-based Risk Identification Benchmark",
          "venue": "ICRA 2024",
          "authors": [
            "Chi-Hsi Kung",
            "Chieh-Chi Yang",
            "Pang-Yuan Pao",
            "Shu-Wei Lu",
            "Pin-Lun Chen",
            "Hsin-Cheng Lu",
            "Yi-Ting Chen"
          ],
          "links": {
            "paper": "https://arxiv.org/pdf/2312.01659.pdf",
            "projectPage": "https://hcis-lab.github.io/RiskBench/",
            "code": "https://github.com/HCIS-Lab/RiskBench"
          }
        },
        {
          "title": "Action-Slot: Visual Action-centric Representation for Atomic Activity Recognition in Traffic Scenes",
          "venue": "CVPR 2024",
          "authors": [
            "Chi-Hsi Kung",
            "Shu-Wei Lu",
            "Yi-Hsuan Tsai",
            "Yi-Ting Chen"
          ],
          "links": {
            "paper": "https://arxiv.org/pdf/2311.17948.pdf",
            "projectPage": "https://hcis-lab.github.io/Action-slot/",
            "code": "https://github.com/HCIS-Lab/Action-slot/tree/main"
          }
        },
        {
          "title": "DROID: Driver-centric Risk Object Identification",
          "venue": "TPAMI 2023",
          "authors": ["C. Li", "S. H. Chan", "Y.-T. Chen"],
          "links": {
            "paper": "https://arxiv.org/pdf/2106.13201.pdf",
            "ieee": "https://ieeexplore.ieee.org/document/10177972"
          }
        },
        {
          "title": "Ordered Atomic Activity for Fine-grained Interactive Traffic Scenario Understanding",
          "venue": "ICCV 2023",
          "authors": ["Nakul Agarwal", "Yi-Ting Chen"],
          "links": {
            "paper": "https://drive.google.com/file/d/1Jwzzr0puAWte5xa-xQwOAnpAXsBsSw7f/view",
            "projectPage": "https://usa.honda-ri.com/oats"
          }
        },
        {
          "title": "Orbeez-SLAM: A Real-time Monocular Visual SLAM with ORB Features and NeRF-realized Mapping",
          "venue": "ICRA 2023",
          "authors": [
            "C.-M. Chung",
            "Y.-C. Tseng",
            "Y.-C. Hsu",
            "X.-Q. Shi",
            "Y.-H. Hua",
            "J.-F. Yeh",
            "Y.-T. Chen",
            "W.-C. Chen",
            "Winston Hsu"
          ],
          "links": {
            "paper": "https://arxiv.org/pdf/2209.13274.pdf",
            "code": "https://github.com/MarvinChung/Orbeez-SLAM"
          }
        }
      ]
    },
    {
      "id": "assistive-robotics",
      "name": "Assistive Robotics",
      "description": "Creating robots that assist humans in daily tasks through intelligent perception, manipulation, and human-robot interaction.",
      "color": "#eb984e",
      "icon": "robot",
      "focusAreas": [
        {
          "id": "food-serving",
          "name": "Food Serving Robot",
          "description": "Robots that understand food properties and assist with feeding tasks through active perception and manipulation."
        },
        {
          "id": "manipulation-skills",
          "name": "Learning to Acquire Manipulation Skills",
          "description": "Teaching robots to learn complex manipulation tasks through imitation learning and reward models."
        }
      ],
      "publications": [
        {
          "title": "VICtoR: Learning Hierarchical Vision-Instruction Correlation Rewards for Long-horizon Manipulation",
          "venue": "ICLR 2025",
          "authors": [
            "Kuo-Han Hung",
            "Pang-Chi Lo",
            "Jia-Fong Yeh",
            "Han-Yuan Hsu",
            "Yi-Ting Chen",
            "Winston H Hsu"
          ],
          "links": {}
        },
        {
          "title": "GRITS: A Spillage-Aware Guided Diffusion Policy for Robot Food Scooping Tasks",
          "venue": "arXiv 2025",
          "authors": [
            "Yen-Ling Tai",
            "Yi-Ru Yang",
            "Kuan-Ting Yu",
            "Yu-Wei Chao",
            "Yi-Ting Chen"
          ],
          "links": {
            "paper": "https://arxiv.org/pdf/2510.00573"
          }
        },
        {
          "title": "SKT-Hang: Hanging Everyday Objects via Object-Agnostic Semantic Keypoint Trajectory Generation",
          "venue": "ICRA 2024",
          "authors": ["Chia-Liang Kuo", "Yu-Wei Chao", "Yi-Ting Chen"],
          "links": {
            "paper": "https://arxiv.org/pdf/2312.04936.pdf",
            "projectPage": "https://hcis-lab.github.io/SKT-Hang/",
            "code": "https://github.com/HCIS-Lab/SKT-Hang",
            "nvidia": "https://developer.nvidia.com/blog/nvidia-presents-new-robotics-research-on-geometric-fabrics-surgical-robots-and-more-at-icra/"
          }
        },
        {
          "title": "SCONE: A Food Scooping Robot Learning Framework with Active Perception",
          "venue": "CoRL 2023",
          "authors": [
            "Yen-Ling Tai",
            "Yu Chien Chiu",
            "Yu-Wei Chao",
            "Yi-Ting Chen"
          ],
          "links": {
            "paper": "https://openreview.net/pdf?id=yHlUVHWnBN",
            "projectPage": "https://sites.google.com/view/corlscone",
            "code": "https://github.com/HCIS-Lab/SCONE"
          }
        },
        {
          "title": "Stage Conscious Attention Network (SCAN): A Demonstration-conditioned Policy for Few-shot Imitation",
          "venue": "AAAI 2022",
          "authors": [
            "Jia-Fong Yeh",
            "Chi-Ming Chung",
            "Hung-Ting Su",
            "Y.-T. Chen",
            "Winston H. Hsu"
          ],
          "links": {
            "paper": "https://arxiv.org/pdf/2112.02278.pdf",
            "projectPage": "https://sites.google.com/view/scan-aaai2022"
          }
        }
      ]
    }
  ],
  "recentNews": [
    {
      "date": "2026-01",
      "text": "Four papers are accepted at ICRA'26!",
      "highlight": true
    },
    {
      "date": "2025-06",
      "text": "Two papers on Visual Representation Learning and Dexterous Hand Manipulation are accepted at ICCV'25!",
      "highlight": true
    },
    {
      "date": "2025-06",
      "text": "One paper on Aerial Traffic Scene Understanding is accepted at IROS'25!",
      "links": {
        "paper": "https://arxiv.org/abs/2503.18553"
      }
    },
    {
      "date": "2025-05",
      "text": "We will host the first X-Sense workshop: Ego-Exo Sensing for Smart Mobility at ICCV 2025 in Honolulu, Hawaii!",
      "links": {
        "website": "https://x-sense-ego-exo.github.io/"
      }
    },
    {
      "date": "2025-04",
      "text": "One paper on Articulated Object Manipulation working with Prof. David Held @CMU is accepted at RSS'25!"
    },
    {
      "date": "2025-02",
      "text": "Two papers on BEV Perception and Radar-Camera Calibration are accepted at CVPR'25!"
    },
    {
      "date": "2025-01",
      "text": "A paper on Learning Scene Affordance for Visual-ROI is accepted at ICRA'25!"
    },
    {
      "date": "2025-01",
      "text": "A paper on Learning Reward Models from Vision and Language is accepted at ICLR'25!"
    },
    {
      "date": "2024-09",
      "text": "A paper on Adaptable Error Detection for Few-shot Imitation is accepted at NeurIPS'24!"
    },
    {
      "date": "2024-06",
      "text": "Our proposal to NSTC 2030 International Outstanding Young Scholars program is granted!",
      "highlight": true
    }
  ],
  "visualizationConfig": {
    "ecosystem": {
      "environment": {
        "label": "Environment (Agent)",
        "color": "#f5d76e",
        "description": "The intelligent system context that encompasses all interactions"
      },
      "robots": {
        "label": "Robots",
        "color": "#95a5a6",
        "description": "Autonomous systems with sensing, planning, and actuation capabilities"
      },
      "caregiver": {
        "label": "Caregiver",
        "color": "#5dade2",
        "description": "Human operators who interact with and guide the robotic systems"
      },
      "careRecipient": {
        "label": "Care Recipient",
        "color": "#eb984e",
        "description": "End users who benefit from the human-robot collaboration"
      }
    },
    "humanCentered": {
      "robot": {
        "label": "Robot",
        "color": "#5dade2",
        "description": "Hardware and system capabilities designed around human needs"
      },
      "human": {
        "label": "Human",
        "color": "#f5d76e",
        "description": "Central to all design decisions - the focus of human-centered AI"
      }
    }
  }
}
